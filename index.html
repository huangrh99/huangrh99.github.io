<!DOCTYPE html>
<!-- saved from url=(0073)file:///D:/yesti/Downloads/Compressed/yestinl.github.io-master/index.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title> Runhui Huang (黄润辉) | The University of Hong Kong </title>
    <!-- link rel="icon" href="images/nlp_logo.jpg" / -->
    <meta name="author" content="Runhui Huang">
    <meta name="keywords" content="Runhui Huang, 黄润辉, Runhui Huang HKU, SYSU, CV, NLP, MultiModal">
    <meta name="robots" content="index,follow">
    <meta name="description" content="Homepage of Runhui Huang">

    <link rel="stylesheet" href="./index_files/styles.css">
    <link href="./index_files/css" rel="stylesheet" type="text/css">
</head>


<body>
<div class="container content">
    <main>
        <div class="home">

            <div class="mini-intro">
                <img class="avatar" src="./index_files/huangrunhui.jpg" alt="huangrh9-photo">

                <h1 id="huangrh9">Runhui Huang <span style="font-family:STFangsong">(黄润辉)</span></h1>
                <p><small><code class="language-plaintext highlighter-rouge">huangrh9@gmail.com</code></small></p>

                <p>Hi, I am currently a PhD student at the University of Hong Kong, under the supervision of <a href="https://hszhao.github.io/">Prof. Hengshuang Zhao</a>. Previously, I obtained my Master's degree supervised by <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ&amp;hl=zh-CN">Prof. Xiaodan Liang</a> and my Bachelor's degree from Sun Yat-sen University in 2024 and 2021, respectively. 

                </p><p>I have a broad interest in vision-language pre-training and post-training, particularly in applying deep learning to practical tasks.
                    Currently, my research focuses on <span style="color:#B08519">Unified Multimodal Understanding and Generation Models</span>.</p>

                <p>More about me: <a href="https://scholar.google.com/citations?user=B5zcj4wAAAAJ">Google Scholar</a> / <a href="https://github.com/huangrh99">Github</a> </p>

                <hr>

                <h3 id="research-interests">Research Interests</h3>
                    <ul>
                        <li>Unified Multimodal Understanding and Generation Models</li>
                        <li>Unified RL training for Unified Multimodal Models</li>
                    </ul>

                    <!--
                    <h3 id="news">News</h3>
                      <ul>
                        <li><small> <span style="color:#B08519">[Oct 2020]</span> x </small></li>
                      </ul>
                    -->

                    <h3 id="publications">Selected Publications <font size="4.5">[<a href="https://scholar.google.com/citations?hl=en&user=B5zcj4wAAAAJ">Google Scholar</a>]</font></h3>
                    <ul>

                        <li>
                          <div id="text" style="color:#1f57b8">Illume+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement</div>
                          <strong>Runhui Huang</strong>, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu
                          <br><i>ArXiv Preprint</i>, 2025.
                          [<a href="https://arxiv.org/abs/2504.01934">paper</a>] [<a href="https://illume-unified-mllm.github.io/">project</a>] [<a href="https://github.com/illume-unified-mllm/ILLUME_plus">code</a>] [<a href="https://huggingface.co/spaces/ILLUME-MLLM/ILLUME_plus-7b">demo</a>] [<a href="https://huggingface.co/collections/ILLUME-MLLM/illume-models-683b3916f5af2d0a015b3477">models</a>]
                        </li>


                        <li>
                          <div id="text" style="color:#1f57b8">Emova: Empowering language models to see, hear and speak with vivid emotions</div>
                          Kai Chen*, Yunhao Gou*, <strong>Runhui Huang*</strong>, Zhili Liu*, Daxin Tan*, Jing Xu, Chao Wang, Yi Zhu, Yihan Zeng.
                          <br><i>CVPR</i>, 2025.
                          [<a href="https://arxiv.org/abs/2409.18042">paper</a>] [<a href="https://emova-ollm.github.io/">project</a>] [<a href="https://github.com/emova-ollm/EMOVA">code</a>]
                        </li>

                        <li>
                          <div id="text" style="color:#1f57b8">HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models</div>
                          <strong>Runhui Huang</strong>, Xinpeng Ding, Chunwei Wang, Jianhua Han, Yulong Liu, Hengshuang Zhao, Hang Xu, Lu Hou, Wei Zhang, Xiaodan Liang.
                          <br><i>CVPR</i>, 2025.
                          [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_HiRes-LLaVA_Restoring_Fragmentation_Input_in_High-Resolution_Large_Vision-Language_Models_CVPR_2025_paper.pdf">paper</a>] 
                        </li>

                        <li>
                          <div id="text" style="color:#1f57b8">LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model</div>
                          <strong>Runhui Huang</strong>, Kaixin Cai, Jianhua Han, Xiaodan Liang, Renjing Pei, Guansong Lu, Songcen Xu, Wei Zhang, Hang Xu
                          <br><i>ECCV</i>, 2024.
                          [<a href="https://arxiv.org/abs/2403.11929">paper</a>] 
                        </li>


                        <!-- <li>
                          <div id="text" style="color:#1f57b8">DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability</div>
                          <strong>Runhui Huang</strong>, Jianhua Han, Guansong Lu, Xiaodan Liang, Yihan Zeng, Wei Zhang, Hang Xu.
                          <br><i>International Conference on Computer Vision</i>, 2023.
                          [<a href="https://arxiv.org/abs/2308.09306.pdf">paper</a>] 
                        </li>

                        <li>
                          <div id="text" style="color:#1f57b8">NLIP: Noise-robust Language-Image Pre-training</div>
                          <strong>Runhui Huang</strong>, Yanxin Long, Jianhua Han, Hang Xu, Xiwen Liang, Chunjing Xu, Xiaodan Liang.
                          <br><i>AAAI Conference on Artificial Intelligence</i>, 2023.
                          [<a href="https://arxiv.org/pdf/2212.07086.pdf">paper</a>] 
                        </li> -->

                        <li>
                            <div id="text" style="color:#1f57b8">FILIP: FINE-GRAINED INTERACTIVE LANGUAGEIMAGE PRE-TRAINING</div>
                            Lewei Yao∗, <strong>Runhui Huang∗</strong>, Lu Hou∗, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu. 
                            <br><i>International Conference on Learning Representations (ICLR)</i>, 2022. 
                            [<a href="https://arxiv.org/pdf/2111.07783.pdf">paper</a>] 
                        </li>
                        <p> </p>

                    </ul>

                    <!-- <h3 id="preprint">Preprint</h3>
 	    <ul>
                        <li>
                            <div id="text" style="color:#1f57b8">Towards Deviation-robust Agent Navigation via Perturbation-aware Contrastive Learning</div>
                            <strong>Yanxin Long*</strong>, Bingqian Lin*, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin.
                            <br><i>IEEE Transactions on Pattern Analysis and Machine Intelligence,</i>, 2021. (<b>T-PAMI 2021, IF: 16.389</b>)(under review) </li>

                        <p> </p>

                    </ul>
	    <ul>
                        <li>
                            <div id="text" style="color:#1f57b8">Prompt-Enhanced Self-Training for Open-Vocabulary Object Detection</div>
                            <strong>Yanxin Long</strong>, Jianhua Han, Runhui Huang, Yi Zhu, Xiaodan Liang, Hang Xu, Chunjing Xu.
                            <br><i>International Conference on Machine Learning,</i>, 2021. (<b>ICML 2022, CCF A</b>)(under review) </li>

                        <p> </p>

                    </ul> -->
                    
                    <!-- <h3 id="awards">Experiences</h3>
                    <ul>
                        <li> <strong>June 2021-May 2022</strong>, Research Intern, Noah Ark Lab, Huawei. </li>
                    </ul> -->
        
                    <!-- <h3 id="awards">Honors &amp; Awards</h3>
                    <ul>
                        <li> Guangdong Guangda Scholarships for Further Studies, 2021 </li>
                        <li> Outstanding Student of Sun Yat-sen University, SYSU, 2018, 2019 </li>
                    </ul> -->
	<h3 id="professional">Professional Activities</h3>
                    <ul>
                        <li> Reviewer: CVPR2023, CVPR2024, CVPR2025, CVPR2026, ICCV2023, ECCV2024, ICCV2025, ECCV2026, ICLR2025, ICLR2026, NeurIPS2024, NeurIPS2025 </li>
                    </ul>
	<!-- <h3 id="hobbies">Hobbies </h3>
                    <ul>
                        <li> GenShin Impact</li>
                    </ul> -->
            </div>

            <div id="footer">
                <p style="text-align:right;">Last modified date: Jan, 2026
                    <br><a href="https://nlp.stanford.edu/~johnhew/">Website template credits</a>
                </p>
                <div style="text-align: center;">
                    <!-- ClustrMaps Widget - Replace the src with your own from clustrmaps.com if needed -->
                    <div id="clustrmaps-widget" style="width: 500px; display: inline-block;">
                        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=pf1USy17kVTDU1HRwmTdkLTgPKWwecDl2slUsbkteWI&cl=ffffff&w=a"></script>
                    </div>
                    <script>
                        // Prevent ClustrMaps from redirecting on click
                        var checkMap = setInterval(function() {
                            var container = document.getElementById('clustrmaps-widget');
                            var link = container ? container.querySelector('a') : null;
                            if (link) {
                                link.removeAttribute('href');
                                link.style.cursor = 'default';
                                link.onclick = function(e) { e.preventDefault(); };
                                clearInterval(checkMap);
                            }
                        }, 500);
                    </script>
                </div>
            </div>



</div></main></div></body></html>
